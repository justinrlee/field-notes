package io.justinrlee.streams.protoprocessor;


import org.apache.kafka.clients.admin.NewTopic;
import org.apache.kafka.common.serialization.Serde;
import org.apache.kafka.common.serialization.Serdes;
import org.apache.kafka.streams.kstream.KStream;
import org.apache.kafka.streams.KafkaStreams;
import org.apache.kafka.streams.KeyValue;
import org.apache.kafka.streams.StreamsBuilder;
import org.apache.kafka.streams.StreamsConfig;
import org.apache.kafka.streams.Topology;
import org.apache.kafka.streams.kstream.Named;
import org.apache.kafka.streams.kstream.Consumed;
import org.apache.kafka.streams.kstream.Produced;
import org.slf4j.Logger;
import org.slf4j.LoggerFactory;

import java.io.FileInputStream;
import java.io.InputStream;
import java.util.Map;
import java.util.HashMap;
import java.util.Arrays;
import java.util.Optional;
import java.util.Properties;
import java.util.concurrent.CountDownLatch;
import java.util.List;
import java.util.stream.Collectors;

import io.justinrlee.streams.protoprocessor.serialize.PersonSerializer;
import io.justinrlee.streams.protoprocessor.serialize.PersonDeserializer;

import com.google.protobuf.util.JsonFormat;

// Auto generated by protoc-jar-maven-plugin
import io.justinrlee.mqtt.protobuf.Person;

import io.confluent.kafka.serializers.AbstractKafkaSchemaSerDeConfig;
import io.confluent.kafka.streams.serdes.avro.SpecificAvroSerde;
import io.confluent.kafka.streams.serdes.protobuf.KafkaProtobufSerde;


/**
 * Hello world!
 *
 */
public class App 
{
    private static final Logger logger = LoggerFactory.getLogger(App.class);

    static void runKafkaStreams(final KafkaStreams streams) 
    {
        final CountDownLatch latch = new CountDownLatch(1);
        streams.setStateListener((newState, oldState) -> 
        {
            if (oldState == KafkaStreams.State.RUNNING && newState != KafkaStreams.State.RUNNING) 
            {
                latch.countDown();
            }
        });

        streams.start();

        try {
            latch.await();
        } catch (final InterruptedException e) {
            throw new RuntimeException(e);
        }

        logger.info("Streams Closed");
    }

    static Map<String, String> generateSerdeConfig(Properties properties) {
      Map<String, String> serdeConfig = new HashMap<String, String>();
      serdeConfig.put("schema.registry.url", properties.getProperty("schema.registry.url"));
      serdeConfig.put("schema.registry.basic.auth.user.info", properties.getProperty("schema.registry.basic.auth.user.info"));
      serdeConfig.put("basic.auth.credentials.source", properties.getProperty("basic.auth.credentials.source"));
      return serdeConfig;
    }

    static Topology buildTopology(String inputTopic, Properties properties)
    {
        Serde<String> stringSerde = Serdes.String();
        Serde<Person> personRawProtobufSerde = Serdes.serdeFrom(new PersonSerializer(), new PersonDeserializer());
        
        KafkaProtobufSerde<Person> personProtobufSerde = new KafkaProtobufSerde<Person>();
        personProtobufSerde.configure(generateSerdeConfig(properties), false);

          // .configure(generateSerdeConfig(properties), false);

        JsonFormat.Printer printer = JsonFormat.printer();

        StreamsBuilder builder = new StreamsBuilder();

        KStream<String, Person> rawProtoStream = builder
            .stream(inputTopic, Consumed.with(stringSerde, personRawProtobufSerde).withName("Raw_Protobuf_Input"))
            .peek((k,v) -> logger.info("Observed event: {}", v), Named.as("Raw_Protobuf_Peek"));

        rawProtoStream
            .to(inputTopic + "-protobuf-sr", Produced.with(stringSerde, personProtobufSerde).withName("Protobuf_SR_Output"));
        
        rawProtoStream
            .map((key, person) ->
                {
                    try { 
                        return new KeyValue<>(key, printer.print(person));
                    } catch (com.google.protobuf.InvalidProtocolBufferException ex) {
                        System.out.println("Something went wrong");
                        return null;
                    }
                },
                Named.as("Proto_To_JSON")
            )
            .to(inputTopic + "-json", Produced.with(stringSerde, stringSerde).withName("JSON_Output"));


        return builder.build();
    }

    public static void main( String[] args ) {

        if (args.length < 1) {
            throw new IllegalArgumentException("This program takes one argument: the path to a configuration file.");
        }

        System.out.println( "Hello World!" );
        Properties props = new Properties();

        try (InputStream inputStream = new FileInputStream(args[0])) {
            props.load(inputStream);
        } catch (Exception e) {
            logger.error("Something went wrong: ", e);
            System.exit(1);
        }

        Topology topology = buildTopology("mqtt", props);
        System.out.println("Topology:");
        System.out.println(topology.describe());

        KafkaStreams kafkaStreams = new KafkaStreams(
            topology,
            props
        );

        Runtime.getRuntime().addShutdownHook(new Thread(kafkaStreams::close));

        logger.info("Kafka Streams 101 App Started");
        runKafkaStreams(kafkaStreams);
    }
}
